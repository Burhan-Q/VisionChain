{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Evaluation","text":""},{"location":"#vision-chain","title":"Vision Chain","text":"<p>Constraining artificial stupidity. </p> <p>It's often surprisingly difficult to train out a stubborn false positive. These false positives can appear very stupid to clients. It can be a simple thing like an object of a given colour is NEVER some class, or that an object of a given size is NEVER another. These problems are simple and these mistakes should not be made, and yet neural networks are not good at learning such absolute, deterministic rules.</p> <p>Good tooling to suggest sensible heuristics, to correct for these false positives does not exist, and this is one problem (among many others), that VisionChain aims to solve. </p> <p>This same style of heuristics are also extremely helpful for both labelling data and for online active-learning in resource constrained environments (other techniques include frame-frame jitter, and threshold based sampling).</p> <p>Similarly there may be a very simple condition for when you should not predict, such as excessive blur, or insufficient exposure. Some business requirements demand that your product must always predict, but many others require high specificity and demand that when your model predicts, it must predict well (potentially due to a high cost automatic intervention). </p> <p>A neural network based object detector can be used to improve the rules (via analysis of its predictions), while the rules can be used to improve the neural network (by increasing the size of the dataset).</p> <p>Most practitioners take inspiration from systems like Tesla's Data Engine, building ever larger dataset in order to teach Neural Networks simple rules, but most practical Machine Learning problems are not this open-ended. Most involve a set of fixed cameras in which object sizes are relatively consistent, and most deployments are aimed at solving a business application, which is not sufficiently addressed by just identifying objects, but instead is resolved by recognising an unexpected combination of items within a certain distance from each other, or the presence of one object in the absense of another etc.</p> <p>Most applications are composite problems, to build a complete product you must both recognise a region of interest and then detect all objects within it, or flag the presence of one object in the absence of another etc.</p> <p>The main downside of such approaches is the manual time to develop sensible rules, but with well-designed software, this need not be so. Rules can be suggested by analysis of a COCO dataset, and accepted or rejected by a developer. </p> <p>With the recent development of 'foundational models', there will be huge growth in 'training free' deployments, where a model (like grounding dino) is sufficiently accurate to deploy for a problem, but requires some guard rails specific to the dataset at hand. Models like GPT required LangChain, now models like GroundingDino need VisionChain. In an environment where GPU demand looks likely to continue to outstrip supply, such techniques will be needed to continue the democratization of Deep Learning applications. </p> <p>Below is a glimpse at the API: </p> <pre><code>preprocessor = Preprocessor([\n  NoPredictFilter(Blur(max_value=0.1)),\n  NoPredictFilter(Exposure(max_value=0.7, min_value=0.2)),\n])\n</code></pre> <pre><code>postprocessor = Postprocessor([\n    Thresholding(thresholds={'person': 0.5, 'car': 0.5, 'truck': 0.5, 'road': 0.5}),\n    ClassAgnosticNMS(nms_threhold=0.8),\n    ShapeFilter(min_width=400, min_height=400, class='car'),\n    ColourFilter(central_colour='XXX', range='XXX', class='car'),\n    OnlyPredictInsideRegionFilter(region_defining_classes=['road'])\n])\n</code></pre> <pre><code>model = Model(\n    preprocessor=preprocessor,\n    model_path='model.onnx',\n    postprocessor=postprocessor,\n    class_list=class_list,\n)\n</code></pre> <p>The demo would be me using this tooling to apply foundation models to a non-coco dataset and business problem in real-time. </p>"},{"location":"active-learning/active-learning/","title":"Active Learning","text":""},{"location":"active-learning/active-learning/#active-learning","title":"Active Learning","text":"<p>Active learning is the problem of choosing what datapoints to label in order to most improve model performance. In most cases it boils down to trying to  find cases where the model is likely to fail, without requiring human input. </p> <p>Most research is focused on offline approaches, taking a large sample of unlabelled images, and ranking them, but often the most valuable  application of Active Learning is in edge deployments, with limited internet bandwidth to send data back to the cloud ready for re-training, and limited computational resource to run the algorithm on. </p>"},{"location":"active-learning/active-learning/#online-approaches","title":"Online Approaches","text":"<ul> <li>Frame-Frame Jitter.</li> <li>Disagreement between constituents of an ensemble.</li> <li>Uncertainty based (range of scores close to the class threshold).</li> <li>Pre-NMS internal disagreement (e.g. both a truck and person, before NMS resolved the predictions via. Score)</li> </ul>"},{"location":"analysis/discovery/","title":"Dataset Analysis","text":"<p>Simple analysis of a dataset in order to come up with sensible heuristics, e.g. min and max size (maybe ignore a few outliers). </p> <p>Similar for colour as well. </p> <p>Also probably possible to find a way to measure how similar shapes are (rotated and resized). </p>"},{"location":"analysis/heuristics/","title":"Heuristics","text":""},{"location":"analysis/heuristics/#colour-based","title":"Colour Based","text":"<ul> <li>This at least needs to be a range</li> </ul>"},{"location":"analysis/heuristics/#shape-based","title":"Shape Based","text":"<ul> <li>Height </li> <li>Width </li> <li>Exact shape from segmentation</li> </ul>"},{"location":"analysis/heuristics/#composite-based","title":"Composite Based","text":"<ul> <li>Includes X and Y in whatever volumes, roughly X percent Orange, and Y percent brown. </li> </ul>"},{"location":"ensembles/aggregations/","title":"Aggregations","text":""},{"location":"ensembles/aggregations/#non-max-suppression","title":"Non-Max Suppression","text":""},{"location":"ensembles/aggregations/#class-agnostic-non-max-suppression","title":"Class Agnostic Non-Max Suppression","text":""},{"location":"ensembles/aggregations/#weighted-box-fusion","title":"Weighted Box Fusion","text":""},{"location":"ensembles/aggregations/#class-preferential-non-max-suppression","title":"Class Preferential Non-Max Suppression","text":""},{"location":"ensembles/ensembles/","title":"Ensembles","text":""},{"location":"ensembles/ensembles/#ensembles","title":"Ensembles","text":"<p>Ensembles for object detection are rarely deployed in production, however it can be very useful to use them when labelling. </p> <p>This will also be the approach used to combine heuristics with neural networks, which would definitely be sufficiently  resource efficient to deploy to production. </p>"},{"location":"ensembles/ensembles/#_1","title":"Ensembles","text":"<pre><code>model = Ensemble(\n  models=[\n    Model(),\n    Model()\n  ],\n  aggregation=[],\n)\n</code></pre>"},{"location":"ensembles/ensembles/#class-list-ensemble","title":"Class List Ensemble","text":"<p>This setup enables you to share the preprocessing actions between each of the models in the ensemble.  But also to cleanly combine the outputs of multiple models.</p> <pre><code>model = Ensemble(\n  preprocessor=Preprocessor(...),\n  models=[\n    Model(\n      model_path='model_one.onnx',\n      postprocessor=postprocessor(...),\n      class_list=['truck','car'],\n    ),\n    Model(\n      model_path='model_two.onnx',\n      postprocessor=postprocessor(...),\n      class_list=['person'],\n    )\n  ],\n  aggregation=NMS(...),\n  postprocessor=Postprocessor([\n\n  ]),\n)\n</code></pre>"},{"location":"heuristics/heuristics/","title":"Heuristics","text":""},{"location":"heuristics/heuristics/#heuristics","title":"Heuristics","text":""},{"location":"heuristics/heuristics/#how-to-build-a-model-out-of-heuristics","title":"How to build a model out of Heuristics","text":"<pre><code>@dataclass\nclass AluminumCan(ClassHeuristic):\n  brightness: float \n  colour_ranges: \n</code></pre> <pre><code>@dataclass\nclass AluminumCan(CompositeHeuristic):\n\n  Brightness(): \n</code></pre> <pre><code>aluminum_can = Heuristic([\n  ShapeFilter(\n      min_width=0,\n      max_width=100,\n      min_height=0,\n      max_height=100,\n  )\n  Brightness(min='', max='') | Edginess(min='', max=''),\n  Cornerness(min='', max=''), \n  Circleness(min='', max=''),\n  Squareness(min='', max=''),\n  Reflectivity(min='', max=''),\n  Transparency(min='', max=''),\n])\n</code></pre> <p>Don't know when this becomes prohibitively slow to compute. </p> <p>2 design options:  * Any list in a list, could be treated as an OR (I think this works?). * Or use the pipe for or, ideally don't want to have to use anything for &amp;, because  that should be the default. Would be quite nice to use something like an arrow based system though.  * Or your own:</p> <pre><code>Any(),\nOr(), #\u00a0or is not the same as all any, because it's one OR the other, not both -&gt; only defined for two options.\nAll()\nNot()\n</code></pre> <p>system</p> <ul> <li> <p>Also need to find a way to do NotX.</p> </li> <li> <p>Measures (https://kornia.readthedocs.io/en/latest/feature.html): </p> </li> <li>kornia.feature.gftt_response</li> <li>Somehow include Hu Moments (how you'd recognise a circle or square)</li> </ul>"}]}